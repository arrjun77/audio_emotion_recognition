{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "decb14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8ac35b7-4c5d-4406-9189-a9f9a7091891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if os.path.exists(\"mfcc_data.pkl\"):\n",
    " #   os.remove(\"mfcc_data.pkl\")\n",
    "  #  print(\"Removed corrupted mfcc_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14046fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from 'mfcc_data.pkl'\n",
      "Feature shape: (2452, 150, 40)\n"
     ]
    }
   ],
   "source": [
    "audio_folder = r\"A:\\video_project\\data\\raw\\audio\"\n",
    "\n",
    "\n",
    "preprocessed_file = \"mfcc_data.pkl\"\n",
    "\n",
    "\n",
    "load_from_file = True\n",
    "\n",
    "\n",
    "label_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "if load_from_file and os.path.exists(preprocessed_file):\n",
    "    with open(preprocessed_file, \"rb\") as f:\n",
    "        X, y, le = pickle.load(f)\n",
    "    print(f\"Loaded preprocessed data from '{preprocessed_file}'\")\n",
    "else:\n",
    "    print(\"Preprocessing audio files...\")\n",
    "\n",
    "    mfcc_features = []\n",
    "    labels = []\n",
    "\n",
    "    for file_name in os.listdir(audio_folder):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            file_path = os.path.join(audio_folder, file_name)\n",
    "            y_audio, sr = librosa.load(file_path, sr=16000)\n",
    "            mfcc = librosa.feature.mfcc(y=y_audio, sr=sr, n_mfcc=40)\n",
    "            if mfcc.shape[1] < 150:\n",
    "                pad_width = 150 - mfcc.shape[1]\n",
    "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                mfcc = mfcc[:, :150]\n",
    "            mfcc_features.append(mfcc.T)\n",
    "            emotion_code = file_name.split(\"-\")[2]\n",
    "            labels.append(label_map.get(emotion_code, 'unknown'))\n",
    "\n",
    "    X = np.array(mfcc_features)\n",
    "    y = np.array(labels)\n",
    "\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    with open(preprocessed_file, \"wb\") as f:\n",
    "        pickle.dump((X, y, le), f)\n",
    "\n",
    "    print(f\"Processed {len(X)} audio files and saved to '{preprocessed_file}'\")\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eb3e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(\"Classes:\", le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7adbba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9ab6f",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d34a1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c2ec454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioEmotionNet(nn.Module):\n",
    "    def __init__(self, input_dim=40, hidden_dim=128, output_dim=8):\n",
    "        super(AudioEmotionNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioEmotionNet(output_dim=len(le.classes_)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ade3f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.9303\n",
      "Epoch 2, Loss: 1.7386\n",
      "Epoch 3, Loss: 1.6431\n",
      "Epoch 4, Loss: 1.5353\n",
      "Epoch 5, Loss: 1.4256\n",
      "Epoch 6, Loss: 1.3295\n",
      "Epoch 7, Loss: 1.2640\n",
      "Epoch 8, Loss: 1.1826\n",
      "Epoch 9, Loss: 1.1211\n",
      "Epoch 10, Loss: 1.0592\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8900d759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.74        75\n",
      "           1       0.62      0.92      0.74        76\n",
      "           2       0.28      0.56      0.38        39\n",
      "           3       0.49      0.57      0.53        67\n",
      "           4       0.80      0.44      0.57        89\n",
      "           5       0.78      0.25      0.38        28\n",
      "           6       0.45      0.54      0.49        71\n",
      "           7       0.35      0.15      0.21        46\n",
      "\n",
      "    accuracy                           0.55       491\n",
      "   macro avg       0.57      0.51      0.50       491\n",
      "weighted avg       0.60      0.55      0.54       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "target_names = [str(c) for c in le.classes_]\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfbe9a15-c182-40f8-9249-e002c2a91275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, max_len=150):\n",
    "    y, sr = librosa.load(file_path, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return torch.tensor(mfcc.T, dtype=torch.float32).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91ce19ce-abd4-476f-83e8-2512d5544f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-01-02-01-01-02-01.wav: neutral\n",
      "03-01-04-01-01-01-17.wav: fearful\n"
     ]
    }
   ],
   "source": [
    "test_folder = r\"A:\\video_project\\data\\raw\\test_audio\"\n",
    "results = []\n",
    "label_map = {\n",
    "    '1': 'neutral',\n",
    "    '2': 'calm',\n",
    "    '3': 'happy',\n",
    "    '4': 'sad',\n",
    "    '5': 'angry',\n",
    "    '6': 'fearful',\n",
    "    '7': 'disgust',\n",
    "    '8': 'surprised'\n",
    "}\n",
    "model.eval()\n",
    "for file in os.listdir(test_folder):\n",
    "    if file.endswith(\".wav\"):\n",
    "        file_path = os.path.join(test_folder, file)\n",
    "        input_tensor = preprocess_audio(file_path).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred_class = torch.argmax(output, dim=1).item()\n",
    "            pred_emotion = str(le.classes_[pred_class])\n",
    "        results.append((file, pred_emotion))\n",
    "\n",
    "# Print results\n",
    "for file_name, emotion in results:\n",
    "    print(f\"{file_name}: {label_map[str(emotion)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db060f18-fea6-43ba-a398-c3a9cfd2ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording saved. Predicting...\n",
      "Predicted Emotion: fearful\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import tempfile\n",
    "\n",
    "def record_audio(duration=3, fs=16000):\n",
    "    print(\"Recording...\")\n",
    "    \n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    \n",
    "    sd.wait()\n",
    "    temp_path = tempfile.mktemp(suffix=\".wav\")\n",
    "    write(temp_path, fs, audio)\n",
    "    \n",
    "    print(\"Recording saved. Predicting...\")\n",
    "    return temp_path\n",
    "\n",
    "\n",
    "test_file = record_audio()\n",
    "input_tensor = preprocess_audio(test_file).to(device)\n",
    "label_map = {\n",
    "    '1': 'neutral',\n",
    "    '2': 'calm',\n",
    "    '3': 'happy',\n",
    "    '4': 'sad',\n",
    "    '5': 'angry',\n",
    "    '6': 'fearful',\n",
    "    '7': 'disgust',\n",
    "    '8': 'surprised'\n",
    "}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    predicted_emotion = le.classes_[predicted_class]\n",
    "print(\"Predicted Emotion:\", label_map[str(predicted_emotion)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63a0de89-a6f6-43f2-bbdd-dccf8df756b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict_emotion_from_upload(file_path):\n",
    "    if not file_path:\n",
    "        return \"No file received. Please upload a .wav file.\"\n",
    "    \n",
    "    try:\n",
    "        input_tensor = preprocess_audio(file_path).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred_class = torch.argmax(output, dim=1).item()\n",
    "            return f\"Predicted Emotion: {label_map[str(le.classes_[pred_class])]}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "gr.Interface(\n",
    "    fn=predict_emotion_from_upload,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Upload or record your voice\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Real-Time Audio Emotion Detector\",\n",
    "    description=\"Upload or record a short audio to detect the emotion\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df06180-3b64-4ddd-b6b7-1aa06946ac10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
